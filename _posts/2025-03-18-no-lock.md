---
layout: post
title: 并发与异步编程（五） -- 无锁编程梳理
categories: 并发与异步编程
tags: CPU 无锁编程
---

* content
{:toc}

异步编程学习实践系列，本篇梳理学习无锁编程。



## 1. 背景

并发编程中离不开资源同步，往往伴随着mutex、semaphore、condition_variable等机制，在实际使用中如果不是对实时性要求极高的场景，合理控制锁粒度和持锁时长基本都能满足业务要求了。

使用锁来进行并发同步时，性能受锁竞争和上下文切换的开销的影响。其中还涉及局部性原理对应的缓存问题，一并进行分析。

若想要更进一步减少锁竞争带来的损耗，可以尝试无锁（lock-free）相关技术和相关结构，本篇开始梳理学习无锁编程相关的技术点。

## 2. 锁竞争的代价

先看看锁竞争的代价，用个demo来实际对比一下。

### 2.1. 多线程加锁访问（实验）

编译：`g++ mutex_cost.cpp -o mutex_cost -pthread`

其中 `std::chrono::high_resolution_clock` 表示最小滴答周期（tick period）的实现，一般会是`std::chrono::system_clock`或者`std::chrono::steady_clock`的别名，也支持第三方实现，具体以不同的编译平台为准。具体见：[cppreference](https://en.cppreference.com/w/cpp/chrono/high_resolution_clock)

```cpp
// mutex_cost.cpp
#include <iostream>
#include <thread>
#include <mutex>
#include <chrono>
#include <vector>

std::mutex mtx;
int shared_variable = 0;

void worker(int iterations) {
    for (int i = 0; i < iterations; ++i) {
        std::lock_guard<std::mutex> lock(mtx);
        // 模拟一些工作
        ++shared_variable;
    }
}

int main() {
    const int num_threads = 4;
    const int iterations = 1000000;

    std::vector<std::thread> threads;
    auto start_time = std::chrono::high_resolution_clock::now();

    // 创建并启动线程
    for (int i = 0; i < num_threads; ++i) {
        threads.emplace_back(worker, iterations);
    }

    // 等待所有线程完成
    for (auto& t : threads) {
        t.join();
    }

    auto end_time = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time).count();

    std::cout << "Total time: " << duration << " ms" << std::endl;
    std::cout << "Final shared variable value: " << shared_variable << std::endl;

    return 0;
}    
```

`perf list`看下跟缓存相关的几个event，并把之前 [CPU及内存调度（一） -- 进程、线程、系统调用、协程上下文切换](https://xiaodongq.github.io/2025/03/09/context-switch) 中谈到的TLB缓存命中率、上下文切换一并统计对比。

```sh
# perf stat -e cache-references,cache-misses,cpu-clock,cycles,instructions,context-switches,L1-dcache-loads,L1-dcache-load-misses,L1-icache-loads,L1-icache-load-misses,dTLB-loads,dTLB-load-misses,iTLB-loads,iTLB-load-misses
[CentOS-root@xdlinux ➜ mutex_cost git:(main) ✗ ]$ perf stat -e cache-references,cache-misses,cpu-clock,cycles,instructions,context-switches,L1-dcache-loads,L1-dcache-load-misses,L1-icache-loads,L1-icache-load-misses,dTLB-loads,dTLB-load-misses,iTLB-loads,iTLB-load-misses ./mutex_cost
Total time: 150 ms
Final shared variable value: 4000000

 Performance counter stats for './mutex_cost':

        # 运行期间对 CPU 缓存的总访问次数
        34,939,239      cache-references          #   73.922 M/sec                    (40.55%)
        # 访问 CPU 缓存时发生缺失的次数
         8,606,333      cache-misses              #   24.632 % of all cache refs      (41.60%)
        # 在 CPU 上实际运行所花费的时钟时间
            472.65 msec cpu-clock                 #    3.112 CPUs utilized          
        #  CPU 所经历的时钟周期数
     1,862,093,362      cycles                    #    3.940 GHz                      (43.42%)
        # 程序执行的指令总数
     1,816,283,768      instructions              #    0.98  insn per cycle           (42.92%)
        # 上下文切换次数
            46,308      context-switches          #    0.098 M/sec                  
       751,711,274      L1-dcache-loads           # 1590.424 M/sec                    (43.25%)
        # L1 数据缓存加载时发生缺失的次数
        12,983,482      L1-dcache-load-misses     #    1.73% of all L1-dcache accesses  (43.18%)
       105,670,987      L1-icache-loads           #  223.572 M/sec                    (42.96%)
        # L1 指令缓存加载时发生缺失的次数
           537,149      L1-icache-load-misses     #    0.51% of all L1-icache accesses  (40.99%)
            12,958      dTLB-loads                #    0.027 M/sec                    (40.48%)
        # 数据TLB 加载时发生缺失的次数
             1,099      dTLB-load-misses          #    8.48% of all dTLB cache accesses  (40.11%)
               892      iTLB-loads                #    0.002 M/sec                    (40.76%)
        # 指令TLB 加载时发生缺失的次数
         2,421,208      iTLB-load-misses          # 271435.87% of all iTLB cache accesses  (39.78%)

       0.151890572 seconds time elapsed

       0.150408000 seconds user
       0.296823000 seconds sys
```

说明：

* 其中`cache-references`表示总的CPU缓存访问次数，`cache-misses`表示缓存缺失次数（即缓存没命中），后面是占比。
* 测试5次，平均耗时`150ms`，`cache-misses`变化不大，基本都在`24%`；`context-switches`上下文切换`46,308`左右（0.097 M/sec）。

### 2.2. 原子变量无锁访问（实验）

使用`atomic`替换`mutex`加锁。

编译：`g++ atomic_cost.cpp -o atomic_cost -pthread`

```cpp
// atomic_cost.cpp
#include <iostream>
#include <thread>
#include <atomic>
#include <chrono>
#include <vector>

std::atomic<int> shared_variable(0);

void worker(int iterations) {
    for (int i = 0; i < iterations; ++i) {
        // 原子自增操作
        ++shared_variable;
    }
}

int main() {
    const int num_threads = 4;
    const int iterations = 1000000;

    std::vector<std::thread> threads;
    auto start_time = std::chrono::high_resolution_clock::now();

    // 创建并启动线程
    for (int i = 0; i < num_threads; ++i) {
        threads.emplace_back(worker, iterations);
    }

    // 等待所有线程完成
    for (auto& t : threads) {
        t.join();
    }

    auto end_time = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time).count();

    std::cout << "Total time: " << duration << " ms" << std::endl;
    std::cout << "Final shared variable value: " << shared_variable.load() << std::endl;

    return 0;
}    
```

perf采集结果说明：

* 测试5次，平均耗时`42ms`左右

```sh
[CentOS-root@xdlinux ➜ mutex_cost git:(main) ✗ ]$ perf stat -e cache-references,cache-misses,cpu-clock,cycles,instructions,context-switches,L1-dcache-loads,L1-dcache-load-misses,L1-icache-loads,L1-icache-load-misses,dTLB-loads,dTLB-load-misses,iTLB-loads,iTLB-load-misses ./atomic_cost
Total time: 42 ms
Final shared variable value: 4000000

 Performance counter stats for './atomic_cost':

         2,626,586      cache-references          #   16.746 M/sec                    (35.33%)
         2,455,763      cache-misses              #   93.496 % of all cache refs      (36.92%)
            156.85 msec cpu-clock                 #    3.591 CPUs utilized          
       714,531,996      cycles                    #    4.555 GHz                      (39.46%)
        73,826,720      instructions              #    0.10  insn per cycle           (42.01%)
                 3      context-switches          #    0.019 K/sec                  
        28,718,275      L1-dcache-loads           #  183.091 M/sec                    (44.56%)
         2,321,953      L1-dcache-load-misses     #    8.09% of all L1-dcache accesses  (46.87%)
           333,198      L1-icache-loads           #    2.124 M/sec                    (47.76%)
             3,581      L1-icache-load-misses     #    1.07% of all L1-icache accesses  (46.49%)
               786      dTLB-loads                #    0.005 M/sec                    (43.97%)
               533      dTLB-load-misses          #   67.81% of all dTLB cache accesses  (41.43%)
                 0      iTLB-loads                #    0.000 K/sec                    (38.86%)
               272      iTLB-load-misses          #    0.00% of all iTLB cache accesses  (36.35%)

       0.043679512 seconds time elapsed

       0.155564000 seconds user
       0.000997000 seconds sys
```

### 2.3. 对比分析

上述结果不大直观，放到一起进行对比说明。

|指标|使用mutex|使用atomic|
|--|--|--|
|总时间（ms）|150|42|
|缓存引用次数|34939239|2626586|
|缓存引用速率（M/sec）|73.922|16.746|
|缓存缺失次数|8606333|2455763|
|缓存缺失率（%）|24.632|93.496|
|CPU时钟时间（msec）|472.65|156.85|
|CPU利用率（CPUs utilized）|3.112|3.591|
|时钟周期数|1862093362|714531996|
|CPU频率（GHz）|3.940|4.555|
|指令总数|1816283768|73826720|
|每周期指令数（insn per cycle）|0.98|0.10|
|上下文切换次数|46308|3|
|上下文切换速率（M/sec）|0.098|0.000019|
|L1数据缓存加载次数|751711274|28718275|
|L1数据缓存加载速率（M/sec）|1590.424|183.091|
|L1数据缓存缺失次数|12983482|2321953|
|L1数据缓存缺失率（%）|1.73|8.09|
|L1指令缓存加载次数|105670987|333198|
|L1指令缓存加载速率（M/sec）|223.572|2.124|
|L1指令缓存缺失次数|537149|3581|
|L1指令缓存缺失率（%）|0.51|1.07|
|dTLB加载次数|12958|786|
|dTLB加载速率（M/sec）|0.027|0.005|
|dTLB缺失次数|1099|533|
|dTLB缺失率（%）|8.48|67.81|
|iTLB加载次数|892|0|
|iTLB加载速率（K/sec）|0.002|0|
|iTLB缺失次数|2421208|272|
|iTLB缺失率（%）|271435.87|0|
|实际经过时间（seconds）|0.151890572|0.043679512|
|用户态CPU时间（seconds）|0.150408000|0.155564000|
|内核态CPU时间（seconds）|0.296823000|0.000997000|

分析说明：

* **总时间**：`150ms` vs `42ms`
    * `mutex`：涉及上下文切换和锁竞争的开销，所以更慢。
    * `atomic`：利用CPU提供的原子操作指令，避免了锁的开销，所以更快。
* 缓存引用次数和速率：`mutex` 的缓存引用次数和速率都远高于 `atomic`
    * `mutex`：涉及多个线程的同步和上下文切换，导致更多的指令和数据访问，从而增加了对 CPU 缓存的引用。
    * `atomic`：操作相对简单，对缓存的访问较少。
* **缓存缺失率**：使用`atomic`时极高，缺失率达93%
    * `mutex`：虽然会增加缓存引用，但由于线程在获取锁时可能会被阻塞，此时CPU可以执行其他任务，缓存中的数据有更多机会被**复用**，从而降低了缓存缺失率。
        * 缓存虽然被复用访问了，但是要注意**伪共享问题（false sharing）**，一般设计cache line对齐
    * `atomic`：操作通常是比较轻量级，但由于多个线程可能同时对原子变量进行操作，会**频繁地更新缓存中的数据**，导致缓存行的频繁失效和重新加载，从而使缓存缺失率大幅上升。
* CPU时钟时间和CPU利用率：`atomic`的CPU时钟时间更短，但CPU利用率更高
    * `mutex`：线程等待锁时会处于阻塞状态，不占用CPU时间，因此CPU时钟时间较长，但CPU利用率相对较低。
    * `atomic`：操作是无锁的，线程可以持续执行，不会因为等待锁而阻塞，所以CPU时钟时间更短，但由于多个线程同时竞争原子变量，会使CPU利用率更高。
* 指令总数（instructions）和每周期指令数（insn per cycle）：`mutex`的指令总数更多，但每周期指令数更高
    * `mutex`：操作涉及锁的获取和释放，需要执行更多的指令来实现线程同步。但由于线程在等待锁时会让出 CPU，CPU 有更多机会进行指令调度，所以每周期指令数较高。
    * `atomic`：操作相对简单，指令总数较少。但由于多个线程同时竞争原子变量，会导致 CPU 频繁地进行指令流水线的刷新和重新调度，从而降低了每周期指令数。
* **上下文切换次数**：`mutex` 的上下文切换次数远高于 `atomic`
    * `mutex`：当一个线程尝试获取被其他线程持有的锁时，会被**阻塞**，此时操作系统会进行上下文切换，将 CPU 资源分配给其他线程。在多个线程竞争锁的情况下，会频繁发生上下文切换，增加了系统开销。
    * `atomic`：操作是无锁的，线程不需要等待锁，因此不会因为锁竞争而导致上下文切换，只有在操作系统进行其他调度时才会发生少量的上下文切换。
* L1 缓存和 TLB 相关指标
    * 总体来说，`mutex`的 L1缓存 和 TLB相关指标 相对较好，而`atomic`的 dTLB缺失率 和 L1数据缓存缺失率 较高。
    * 原因：
        * `mutex`：线程在等待锁时会让出 CPU，缓存中的数据有更多机会被复用，减少了 L1 缓存和 TLB 的缺失率。
        * `atomic`：多个线程同时对原子变量进行操作，会频繁地更新缓存中的数据，导致 L1 缓存和 TLB 的频繁失效和重新加载，从而增加了缺失率。

## 3. 一些优化思路

上面实验对比中，有两个优化点此处进行说明：**降低缓存缺失率** 和 **减少上下文切换**。

### 3.1. 降低缓存缺失率

缓存缺失率高的影响：

* 数据访问延迟增加：缓存缺失发生时CPU要从主存获取数据，这比缓存慢得多。L1缓存访问速度一般几个时钟周期，主存则几十上百时钟周期
* 指令执行效率降低：现代处理器通常采用流水线技术来提高指令执行效率，缓存缺失会导致流水线停顿，因为处理器需要等待数据从主内存中加载到缓存后才能继续执行相关指令
* 系统吞吐量下降：高缓存缺失率会使 CPU 花费更多时间等待数据，而不是执行有用的指令，这会导致系统的吞吐量下降

优化思路：

* 优化数据访问模式：
    * **空间局部性优化**，尽量按数据在内存中的存储顺序进行访问；
    * **时间局部性**，频繁使用的数据，放在靠近循环顶部的位置，以减少缓存缺失。
* 合理设计缓存策略：
    * **选择合适的缓存替换算法**，如`LRU`，能够较好地适应大多数程序的访问模式，提高缓存利用率；
    * **调整缓存容量和分区**，可根据访问频率和重要性分区
* 优化程序代码
    * 减少不必要的内存分配和释放，频繁的内存分配和释放会导致内存碎片，影响缓存的使用效率。可使用内存池、对象池

### 3.2. 减少上下文切换

上下文切换的影响：

* 额外的开销：需要保存当前进程或线程的上下文信息（如寄存器值、程序计数器等），并加载新进程或线程的上下文信息。需要一系列的指令，会消耗CPU和内存产生额外开销
    * 相关开销，在[CPU及内存调度（一） -- 进程、线程、系统调用、协程上下文切换](https://xiaodongq.github.io/2025/03/09/context-switch)中也有了解
* 缓存失效与重建：不同的进程或线程可能使用不同的内存空间，切换到新进程和线程时，原来缓存中的数据可能不再有效，需要重新加载数据到缓存中
    * 如上所述，缓存缺失会导致数据访问延迟增加和指令执行效率下降
* 影响程序的局部性
    * 程序的**局部性原理**指出，程序在执行过程中通常会频繁访问相邻的内存地址和最近使用过的数据
    * 上下文切换会打破这种局部性，新的进程或线程可能具有不同的访问模式，导致缓存无法有效利用，降低了缓存的命中率，进而影响程序性能

优化思路：

* 优化线程数量
    * 分析任务特性：对任务进行分析，确定其是`CPU密集型`还是`I/O密集型`。CPU 密集型任务通常需要较少的线程，因为过多的线程会导致频繁的上下文切换，而 I/O 密集型任务则可以通过增加线程数来提高资源利用率。
    * 使用线程池：线程池可以**限制**线程的创建数量，并对线程进行**复用**。通过合理设置线程池的大小，能够避免线程的频繁创建和销毁，从而减少上下文切换。
* 优化锁的使用
    * **减小锁的粒度**：将大的锁分解为多个小的锁，使得每个锁保护的资源范围更小。这样可以降低多个线程同时竞争锁的概率，减少因获取锁而导致的上下文切换。
        * 例如，在一个包含多个元素的集合中，如果对整个集合加锁，那么每次只有一个线程能够访问集合中的元素。可以将集合分解为多个子集合，每个子集合使用一个独立的锁，这样不同的线程可以同时访问不同的子集合，提高并发度。
    * 避免不必要的锁：有些操作可能在单线程环境下执行，或者在特定的条件下不需要进行同步，此时应该去除不必要的锁，以减少上下文切换
* 采用**无锁数据结构**
    * 无锁数据结构通过使用原子操作和一些特殊的算法来实现数据的并发访问，避免了传统锁机制带来的上下文切换开销。
        * 无锁栈（Lock-Free Stack），基于原子操作（如 `CAS`，Compare-And-Swap）实现，其入栈和出栈操作都使用`CAS`来保证操作的原子性
            * 使用场景：比如在多线程环境下，用于任务调度或内存管理中的临时数据存储。多个线程可以同时向栈中压入任务，或者从栈中弹出任务进行处理
            * std::atomic的`compare_exchange_weak`，可见：[cppreference](https://en.cppreference.com/w/cpp/atomic/atomic/compare_exchange)
        * 无锁队列（Lock-Free Queue），通常使用两个原子指针（头指针和尾指针）以及 CAS 操作来实现。
            * 使用场景：比如在多线程生产者 - 消费者模型中广泛应用，生产者线程可以无锁地将任务放入队列，消费者线程可以无锁地从队列中取出任务进行处理
        * 无锁哈希表、无锁链表
* 优化系统调度
    * 设置线程优先级，根据任务的重要性和紧急程度，为线程设置不同的优先级。调度器会优先调度优先级高的线程，减少低优先级线程的上下文切换。
    * 使用**亲缘性绑定**，将线程绑定到特定的 CPU 核心上，使得线程在执行过程中尽量在同一个 CPU 核心上运行，减少因线程在不同 CPU 核心之间迁移而导致的上下文切换。
        * `taskset` 或 `pthread_setaffinity_np`
* 避免阻塞操作
    * 尽量避免在关键路径上执行阻塞操作，如 I/O 操作、等待锁等。可以采用**异步编程模型**或者使用**非阻塞 I/O** 来减少线程的阻塞，从而降低上下文切换的频率。
        * `io_uring`、`epoll`

### 3.3. 实践tips

记录一些实践建议。

1）比如 [为什么你的高并发锁优化做不好？](https://mp.weixin.qq.com/s/EoW1Y7n_SXAjZtGRtcCeVw)中：

* 基于`分片锁`减小锁的粒度时：将锁的粒度减小，分散到多个分片，并通过哈希映射将竞争分散到不同锁上。
    * 分片锁需要去关注硬件方面的细节，建议`N`（分片数量）取CPU核心数的2到4倍，而且要通过`alignas(64)`来对齐缓存行，这样的话能够避免性能上的暗坑。
    * 如：`alignas(64) std::array<std::mutex, N> shards;`（`template <size_t N>;`）

## 4. 无锁编程技术

上面对比锁和atomic时，已经涉及到无锁相关的使用了，但比较散不成体系，这里再梳理总结下。

在无锁编程中实现无阻塞，有一系列的技术可以实现：`原子操作`（如上面提到的`CAS`）、`内存屏障`、`避免ABA问题`等。（主要参考自：[无锁编程简介（翻译）——译自《An Introduction to Lock-Free Programming》](https://zhuanlan.zhihu.com/p/472861479)）

* 原子操作：RMD（Read-Modify-Write）
    * RMW操作的应用：
        * 例如lightweight mutex（轻量级互斥锁）， recursive mutex（递归互斥锁）和lightweight logging system（轻量级的日志系统），本篇参考文章中都贴了链接，可以进一步了解学习
    * RMW操作，包含C++11下的`std::atomic::fetch_add`（Win32、IOS暂不关注）。
        * 注意C++11中，原子操作标准并不保证这个操作在所有平台下都是无锁的，所以最好能够了解你使用的平台是否符合。
        * 可以通过调用`std::atomic<>::is_lock_free`来确认
    * **CAS（Compare-And-Swap）**循环：可能讨论最多的RMW操作就是compare-and-swap（CAS）
        * 当进行CAS循环时，特殊需要注意的地方使操作必须防止出现**ABA问题**

关于内存模型、缓存一致性协议（MESI），可了解：[无锁编程——从CPU缓存一致性讲到内存模型](https://zhuanlan.zhihu.com/p/642416997)

## 5. 小结


## 6. 参考

* [为什么你的高并发锁优化做不好？](https://mp.weixin.qq.com/s/EoW1Y7n_SXAjZtGRtcCeVw)
* [C++性能榨汁机之无锁编程](https://zhuanlan.zhihu.com/p/38664758)
* [无锁编程简介（翻译）——译自《An Introduction to Lock-Free Programming》](https://zhuanlan.zhihu.com/p/472861479)
* [无锁编程——从CPU缓存一致性讲到内存模型](https://zhuanlan.zhihu.com/p/642416997)
* LLM
