---
layout: post
title: MIT6.824学习笔记（三） -- GFS
categories: MIT6.824
tags: 分布式 GFS
---

* content
{:toc}

MIT6.824（2020）学习笔记，Lecture 3 - GFS。



## 1. 背景

MIT6.824（2020）学习，本篇继续学习：Lecture 3 - GFS。

*说明：本博客作为个人学习实践笔记，可供参考但非系统教程，可能存在错误或遗漏，欢迎指正。若需系统学习，建议参考原链接。*

## 2. GFS论文

论文和找的中文翻译，两者结合看下（部分翻译处还是要对比下英文原文）：

* [GFS论文](https://pdos.csail.mit.edu/6.824/papers/gfs.pdf)
* [《The Google File System》论文翻译（GFS-SOSP2003）](https://blog.mrcroxx.com/posts/paper-reading/gfs-sosp2003/)
* 别人的笔记：[GFS 论文阅读](https://tanxinyu.work/gfs-thesis/)

下面只做部分学习笔记，细节还是要看论文内容，自己也还需要常回头看上面参考链接的论文内容。

### 2.1. 引言

`GFS（Google File System）`是由Google设计并实现的为大规模分布式数据密集型应用程序设计的`可伸缩（scalable）`的分布式文件系统。

`GFS`为在廉价商用设备上运行提供了容错能力，并可以在有大量客户端的情况下提供较高的整体性能。

Google碰到的问题和设计探索：

* 1、设备故障经常发生，系统必须具有持续监控、错误检测、容错与自动恢复的能力
    * 故障包括：应用程序bug、操作系统bug、人为错误和硬盘、内存、插头、网络、电源等设备故障
* 2、文件比传统标准更大，数GB大小的文件十分常见，数据集由数十亿个、总计`数TB`的对象组成，并且还在快速增长，管理数十亿个`几KB`大小的文件是非常不明智的。因此需要重新考虑像I/O操作和block大小等设计和参数。
* 3、实际场景中，大部分文件会以`“追加”（append）`的方式变更（mutate），而非`“覆盖写”（overwrite）`
* 4、`应用程序`和`文件系统API`两者**同时**设计便于提高整个系统的灵活性。
    * 例如，我们放宽了GFS的一致性协议，从而大幅简化了系统，减少了应用程序的负担
    * 我们还引入了一种在不需要额外同步操作的条件下，允许多个客户端并发将数据追加到同一个文件的原子性操作

### 2.2.1. 整体架构

整体架构如下图所示，一个GFS集群包括单个`master（主服务器）`和多个`chunkserver（块服务器）`，并被多个`client（客户端）`访问。每个节点通常为一个运行着用户级服务进程的Linux主机。如果资源允许且可以接受不稳定的应用程序代码所带来的低可靠性，那么可以轻松地在一台机器上同时运行`chunkserver`和`client`。

GFS架构图：  
![gfs-architecture](/images/gfs-architecture.png)

**文件和chunk划分：**

* 文件被划分为若干个固定大小的`chunk（块）`。每个chunk被一个不可变的全局唯一的64位`chunk handle（块标识符）`唯一标识，`chunk handle`在chunk被创建时由主节点分配。
* `chunkserver`将`chunk作`为`Linux文件`存储到本地磁盘中，通过`chunk handle`和`byte range（字节范围）`来确定需要被读写的chunk和chunk中的数据。
* 为了可靠性考虑，每个chunk会在多个`chunkserver`中有**副本**。我们默认存储`三份副本`，用户也可以为不同的命名空间的域指定不同的副本级别。
* chunk大小是关键的设计参数之一。GFS选择了`64MB`，其远大于通常的文件系统的块大小。选择较大的chunk大小提供了很多重要的优势。
    * 减少了client与master交互的次数，因为对一个chunk的读写仅需要与master通信一次以请求其位置信息
    * 因为chunk较大，client更有可能在一个chunk上执行更多的操作，这可以通过与chunkserver保持更长时间的TCP连接来减少网络开销
    * 减少了master中保存的元数据大小

**master和元数据：**

* GFS采用单master节点，大大简化了设计。并需最小化master节点在读写中的参与，以避免其成为系统瓶颈。
    * client不会直接从master读取文件数据，而是询问master它需要与哪个chunkserver通信
    * client会在一定时间内缓存信息，并直接与对应的chunkserver通信以完成后续操作。
* master维护系统所有的**元数据**。元数据包括`文件和chunk的命名空间（namespace）`、`文件到chunk的映射`和`chunk每个副本的位置`这3种，此外还有`访问控制（access control）信息`。
    * 所有元数据被存储在master的**内存**中。
    * 前两种类型（命名空间和映射），还通过将`变更（mutation）`记录到一个`操作日志（operation log）`的方式**持久化存储**在master的磁盘上，并在远程机器上备份。
    * 通过`日志`(operation log)，我们可以简单、可靠地更新master的状态，即使master故障也没有数据不一致的风险。
    * master不会持久化存储`chunk的位置信息`，而是在启动时和当chunkserver加入集群时向chunkserver询问其存储的chunk信息。
* master还控制系统级活动如chunk租约（chunk lease）管理、孤儿chunk垃圾回收（garbage collection of orphaned chunks）和chunkserver间的chunk迁移（migration）。master周期性地通过心跳（HeartBeat）消息与每个chunkserver通信，向其下达指令并采集其状态信息。

master通过 **`重放（replay）`操作日志(operation log)**来恢复其文件系统的状态。操作日志要尽可能小以减少启动时间。当日志超过一定大小时，master会对其状态创建一个`检查点（checkpoint）`，这样master就可以从磁盘加载最后一个检查点并重放该检查点后的日志来恢复状态。

检查点的结构为一个紧凑的B树（B-tree）这样它就可以在内存中被直接映射，且在查找命名空间时不需要进行额外的解析。这进一步提高了恢复速度，并增强了系统的可用性。

**GFS client：**

* 被链接到应用程序中的`GFS client`的代码实现了`文件系统API`，并与`master`和`chunkserver`通信，代表应用程序来读写数据。
* 进行**元数据操作**时，client与master交互。而所有的**数据**交互直接由client与chunkserver间进行。GFS不提供`POXIS API`，因此不会陷入到`Linux vnode层`（这里的vnode是抽象概念，对于Linux可以理解成inode和dentry）。

**缓存说明：**

* 无论`client`还是`chunkserver`都不需要缓存文件数据。
* 在client中，因为大部分应用程序需要流式地处理大文件或者数据集过大以至于无法缓存，所以缓存几乎无用武之地。不使用缓存就消除了缓存一致性问题，简化了client和整个系统。（当然，client需要缓存元数据）
* chunkserver中的chunk被作为本地文件存储，Linux系统已经在内存中对经常访问的数据在缓冲区缓存，因此也不需要额外地缓存文件数据。

**一致性模型：**

GFS`宽松`的一致性模型(relaxed consistency model)可以很好地支持我们的高度分布式应用程序，且实现起来简单高效。

文件`命名空间`的变更（例如创建文件）操作是`原子性`的。它们仅由master处理。`命名空间锁`保证了原子性和正确性；master的操作日志定义了这些操作的全局总顺序。

`文件区域（file region）`的状态有下面几种：

* `consistent（一致的）`：如果一个文件区域的任意一个副本被任何client读取总能得到相同的数据，那么这个文件区域状态为consistent（一致的）
* `defined（确定的）`：在一个文件区域的数据变更后，如果它是一致的，且client总能看到其写入的内容，那么这个文件区域的状态为defined（确定的）
    * defined状态包含了consistent状态
* `consistent but undefined（一致的但非确定的）`：文件区域在并发变更执行后的状态为consistent but undefined（一致的但非确定的）
    * 所有客户端能看到同样的数据，但数据可能并不反映任何一个`变更写入`的数据。

通常，数据融合了多个变更的内容。文件区域在一个失败的变更后状态会变为`inconsistent（不一致的）`（且undefined）：不同client在不同时刻可能看到不同的数据。

### 2.2.2. 读操作简要流程

结合上述架构图，说明下“读”操作的流程：

1. 首先，通过固定的chunk大小，client将应用程序指定的`文件名`和`chunk偏移量`翻译为该文件中的`chunk index`（块序号）
2. 然后，client向master发送一个包含了文件名和`chunk index`的请求
3. master会返回其相应的`chunk handle`和副本所在的位置
4. client将这个信息以文件名和`chunk index`为键进行**缓存**
5. client接着向最有可能为最近的副本所在的chunkserver发送请求。请求中指定了`chunk handle`和`byte range`。
6. 之后，client再次读取相同的chunk时不再需要与master交互，直到缓存过期或文件被重新打开

事实上，client通常会在同一个请求中请求多个chunk，master也可以返回包含多个chunk的响应。这种方式避免了client与master进一步的通信，在几乎不需要额外开销的情况下得到更多的信息。


## 3. 小结


## 4. 参考

1、[视频：Lecture 2 - GFS](https://www.bilibili.com/video/BV1R7411t71W?p=3&vd_source=477b80445c7c1a81617bbea3bdf9a3c1)

2、[课程中文翻译：Lecture 03 - GFS](https://mit-public-courses-cn-translatio.gitbook.io/mit6-824/lecture-03-gfs)

3、[GFS 论文阅读](https://tanxinyu.work/gfs-thesis/)

4、[《The Google File System》论文翻译（GFS-SOSP2003）](https://blog.mrcroxx.com/posts/paper-reading/gfs-sosp2003/)

5、[MIT 6.824 2020 视频笔记三：GFS](https://www.qtmuniao.com/2020/03/14/6-824-vidoe-notes-3-gfs/)
