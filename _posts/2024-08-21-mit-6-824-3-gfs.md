---
layout: post
title: MIT6.824学习笔记（三） -- GFS
categories: MIT6.824
tags: 分布式 GFS
---

* content
{:toc}

MIT6.824（2020）学习笔记，Lecture 3 - GFS。



## 1. 背景

MIT6.824（2020）学习，本篇继续学习：Lecture 3 - GFS。

*说明：本博客作为个人学习实践笔记，可供参考但非系统教程，可能存在错误或遗漏，欢迎指正。若需系统学习，建议参考原链接。*

## 2. GFS论文

论文和找的中文翻译，两者结合看下（部分翻译处还是要对比下英文原文）：

* [GFS论文](https://pdos.csail.mit.edu/6.824/papers/gfs.pdf)
* [《The Google File System》论文翻译（GFS-SOSP2003）](https://blog.mrcroxx.com/posts/paper-reading/gfs-sosp2003/)
* 别人的笔记：[GFS 论文阅读](https://tanxinyu.work/gfs-thesis/)

下面只做部分学习笔记，细节还是要看论文内容，自己也还需要常回头看上面参考链接的论文内容。

### 2.1. 引言

`GFS（Google File System）`是由Google设计并实现的为大规模分布式数据密集型应用程序设计的`可伸缩（scalable）`的分布式文件系统。

`GFS`为在廉价商用设备上运行提供了容错能力，并可以在有大量客户端的情况下提供较高的整体性能。

Google碰到的问题和设计探索：

* 1、设备故障经常发生，系统必须具有持续监控、错误检测、容错与自动恢复的能力
    * 故障包括：应用程序bug、操作系统bug、人为错误和硬盘、内存、插头、网络、电源等设备故障
* 2、文件比传统标准更大，数GB大小的文件十分常见，数据集由数十亿个、总计`数TB`的对象组成，并且还在快速增长，管理数十亿个`几KB`大小的文件是非常不明智的。因此需要重新考虑像I/O操作和block大小等设计和参数。
* 3、实际场景中，大部分文件会以`“追加”（append）`的方式变更（mutate），而非`“覆盖写”（overwrite）`
* 4、`应用程序`和`文件系统API`两者**同时**设计便于提高整个系统的灵活性。
    * 例如，我们放宽了GFS的一致性协议，从而大幅简化了系统，减少了应用程序的负担
    * 我们还引入了一种在不需要额外同步操作的条件下，允许多个客户端并发将数据追加到同一个文件的原子性操作

### 2.2. 架构说明

#### 2.2.1. 整体架构

整体架构如下图所示，一个GFS集群包括单个`master（主服务器）`和多个`chunkserver（块服务器）`，并被多个`client（客户端）`访问。每个节点通常为一个运行着用户级服务进程的Linux主机。如果资源允许且可以接受不稳定的应用程序代码所带来的低可靠性，那么可以轻松地在一台机器上同时运行`chunkserver`和`client`。

GFS架构图：  
![gfs-architecture](/images/gfs-architecture.png)

**文件和chunk划分：**

* 文件被划分为若干个固定大小的`chunk（块）`。每个chunk被一个不可变的全局唯一的64位`chunk handle（块标识符）`唯一标识，`chunk handle`在chunk被创建时由主节点分配。
* `chunkserver`将`chunk作`为`Linux文件`存储到本地磁盘中，通过`chunk handle`和`byte range（字节范围）`来确定需要被读写的chunk和chunk中的数据。
* 为了可靠性考虑，每个chunk会在多个`chunkserver`中有**副本**。我们默认存储`三份副本`，用户也可以为不同的命名空间的域指定不同的副本级别。

**master和元数据：**

* 采用单master节点大大简化了设计。并需最小化master节点在读写中的参与，以避免其成为系统瓶颈。
    * client不会直接从master读取文件数据，而是询问master它需要与哪个chunkserver通信
    * client会在一定时间内缓存信息，并直接与对应的chunkserver通信以完成后续操作。
* master维护系统所有的**元数据**。元数据包括`命名空间（namespace）`、`访问控制（access control）信息`、`文件到chunk的映射`和`chunk当前的位置`。
* master还控制系统级活动如chunk租约（chunk lease）管理、孤儿chunk垃圾回收（garbage collection of orphaned chunks）和chunkserver间的chunk迁移（migration）。master周期性地通过心跳（HeartBeat）消息与每个chunkserver通信，向其下达指令并采集其状态信息。

**GFS client：**

* 被链接到应用程序中的`GFS client`的代码实现了`文件系统API`，并与`master`和`chunkserver`通信，代表应用程序来读写数据。
* 进行**元数据操作**时，client与master交互。而所有的**数据**交互直接由client与chunkserver间进行。GFS不提供`POXIS API`，因此不会陷入到`Linux vnode层`（这里的vnode是抽象概念，对于Linux可以理解成inode和dentry）。

**缓存说明：**

* 无论`client`还是`chunkserver`都不需要缓存文件数据。
* 在client中，因为大部分应用程序需要流式地处理大文件或者数据集过大以至于无法缓存，所以缓存几乎无用武之地。不使用缓存就消除了缓存一致性问题，简化了client和整个系统。（当然，client需要缓存元数据）
* chunkserver中的chunk被作为本地文件存储，Linux系统已经在内存中对经常访问的数据在缓冲区缓存，因此也不需要额外地缓存文件数据。

#### 2.2.2. 读操作流程

结合上述架构图，说明下“读”操作的流程：

1. 首先，通过固定的chunk大小，client将应用程序指定的`文件名`和`chunk偏移量`翻译为该文件中的`chunk index`（块序号）
2. 然后，client向master发送一个包含了文件名和`chunk index`的请求
3. master会返回其相应的`chunk handle`和副本所在的位置
4. client将这个信息以文件名和`chunk index`为键进行**缓存**
5. client接着向最有可能为最近的副本所在的chunkserver发送请求。请求中指定了`chunk handle`和`byte range`。
6. 之后，client再次读取相同的chunk时不再需要与master交互，直到缓存过期或文件被重新打开

事实上，client通常会在同一个请求中请求多个chunk，master也可以返回包含多个chunk的响应。这种方式避免了client与master进一步的通信，在几乎不需要额外开销的情况下得到更多的信息。


## 3. 小结


## 4. 参考

1、[视频：Lecture 2 - GFS](https://www.bilibili.com/video/BV1R7411t71W?p=3&vd_source=477b80445c7c1a81617bbea3bdf9a3c1)

2、[课程中文翻译：Lecture 03 - GFS](https://mit-public-courses-cn-translatio.gitbook.io/mit6-824/lecture-03-gfs)

3、[GFS 论文阅读](https://tanxinyu.work/gfs-thesis/)

4、[《The Google File System》论文翻译（GFS-SOSP2003）](https://blog.mrcroxx.com/posts/paper-reading/gfs-sosp2003/)

5、[MIT 6.824 2020 视频笔记三：GFS](https://www.qtmuniao.com/2020/03/14/6-824-vidoe-notes-3-gfs/)
