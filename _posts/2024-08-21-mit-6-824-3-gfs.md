---
layout: post
title: MIT6.824学习笔记（三） -- GFS
categories: MIT6.824
tags: 分布式 GFS
---

* content
{:toc}

MIT6.824（2020）学习笔记，Lecture 3 - GFS。



## 1. 背景

MIT6.824（2020）学习，本篇继续学习：Lecture 3 - GFS。

*说明：本博客作为个人学习实践笔记，可供参考但非系统教程，可能存在错误或遗漏，欢迎指正。若需系统学习，建议参考原链接。*

## 2. GFS论文

论文和找的中文翻译，两者结合看下（部分翻译处还是要对比下英文原文）：

* [GFS论文](https://pdos.csail.mit.edu/6.824/papers/gfs.pdf)
* [《The Google File System》论文翻译（GFS-SOSP2003）](https://blog.mrcroxx.com/posts/paper-reading/gfs-sosp2003/)

下面只做部分学习笔记，细节还是要看论文内容，自己也还需要常回头看上面参考链接的论文内容。

### 2.1. 引言

`GFS（Google File System）`是由Google设计并实现的为大规模分布式数据密集型应用程序设计的`可伸缩（scalable）`的分布式文件系统。

`GFS`为在廉价商用设备上运行提供了容错能力，并可以在有大量客户端的情况下提供较高的整体性能。

Google碰到的问题和设计探索：

* 1、设备故障经常发生，系统必须具有持续监控、错误检测、容错与自动恢复的能力
    * 故障包括：应用程序bug、操作系统bug、人为错误和硬盘、内存、插头、网络、电源等设备故障
* 2、文件比传统标准更大，数GB大小的文件十分常见，数据集由数十亿个、总计`数TB`的对象组成，并且还在快速增长，管理数十亿个`几KB`大小的文件是非常不明智的。因此需要重新考虑像I/O操作和block大小等设计和参数。
* 3、实际场景中，大部分文件会以`“追加”（append）`的方式变更（mutate），而非`“覆盖写”（overwrite）`
* 4、`应用程序`和`文件系统API`两者**同时**设计便于提高整个系统的灵活性。
    * 例如，我们放宽了GFS的一致性协议，从而大幅简化了系统，减少了应用程序的负担
    * 我们还引入了一种在不需要额外同步操作的条件下，允许多个客户端并发将数据追加到同一个文件的原子性操作

### 2.2. 整体架构

整体架构如下图所示，一个GFS集群包括单个`master（主服务器）`和多个`chunkserver（块服务器）`，并被多个`client（客户端）`访问。每个节点通常为一个运行着用户级服务进程的Linux主机。如果资源允许且可以接受不稳定的应用程序代码所带来的低可靠性，那么可以轻松地在一台机器上同时运行`chunkserver`和`client`。

GFS架构图：  
![gfs-architecture](/images/gfs-architecture.png)

**文件和chunk划分：**

* 文件被划分为若干个固定大小的`chunk（块）`。每个chunk被一个不可变的全局唯一的64位`chunk handle（块标识符）`唯一标识，`chunk handle`在chunk被创建时由主节点分配。
* `chunkserver`将`chunk作`为`Linux文件`存储到本地磁盘中，通过`chunk handle`和`byte range（字节范围）`来确定需要被读写的chunk和chunk中的数据。
* 为了可靠性考虑，每个chunk会在多个`chunkserver`中有**副本**。我们默认存储`三份副本`，用户也可以为不同的命名空间的域指定不同的副本级别。
* chunk大小是关键的设计参数之一。GFS选择了`64MB`，其远大于通常的文件系统的块大小。选择较大的chunk大小提供了很多重要的优势。
    * 减少了client与master交互的次数，因为对一个chunk的读写仅需要与master通信一次以请求其位置信息
    * 因为chunk较大，client更有可能在一个chunk上执行更多的操作，这可以通过与chunkserver保持更长时间的TCP连接来减少网络开销
    * 减少了master中保存的元数据大小

**master和元数据：**

* GFS采用单master节点，大大简化了设计。并需最小化master节点在读写中的参与，以避免其成为系统瓶颈。
    * client不会直接从master读取文件数据，而是询问master它需要与哪个chunkserver通信
    * client会在一定时间内缓存信息，并直接与对应的chunkserver通信以完成后续操作。
* master维护系统所有的**元数据**。元数据包括`文件和chunk的命名空间（namespace）`、`文件到chunk的映射`和`chunk每个副本的位置`这3种，此外还有`访问控制（access control）信息`。
    * 所有元数据被存储在master的**内存**中。
    * 前两种类型（命名空间和映射），还通过将`变更（mutation）`记录到一个`操作日志（operation log）`的方式**持久化存储**在master的磁盘上，并在远程机器上备份。
    * 通过`日志`(operation log)，我们可以简单、可靠地更新master的状态，即使master故障也没有数据不一致的风险。
    * master不会持久化存储`chunk的位置信息`，而是在启动时和当chunkserver加入集群时向chunkserver询问其存储的chunk信息。
* master还控制系统级活动如chunk租约（chunk lease）管理、孤儿chunk垃圾回收（garbage collection of orphaned chunks）和chunkserver间的chunk迁移（migration）。master周期性地通过心跳（HeartBeat）消息与每个chunkserver通信，向其下达指令并采集其状态信息。

master通过 **`重放（replay）`操作日志(operation log)**来恢复其文件系统的状态。操作日志要尽可能小以减少启动时间。当日志超过一定大小时，master会对其状态创建一个`检查点（checkpoint）`，这样master就可以从磁盘加载最后一个检查点并重放该检查点后的日志来恢复状态。

检查点的结构为一个紧凑的B树（B-tree）这样它就可以在内存中被直接映射，且在查找命名空间时不需要进行额外的解析。这进一步提高了恢复速度，并增强了系统的可用性。

**GFS client：**

* 被链接到应用程序中的`GFS client`的代码实现了`文件系统API`，并与`master`和`chunkserver`通信，代表应用程序来读写数据。
* 进行**元数据操作**时，client与master交互。而所有的**数据**交互直接由client与chunkserver间进行。GFS不提供`POXIS API`，因此不会陷入到`Linux vnode层`（这里的vnode是抽象概念，对于Linux可以理解成inode和dentry）。

**缓存说明：**

* 无论`client`还是`chunkserver`都不需要缓存文件数据。
* 在client中，因为大部分应用程序需要流式地处理大文件或者数据集过大以至于无法缓存，所以缓存几乎无用武之地。不使用缓存就消除了缓存一致性问题，简化了client和整个系统。（当然，client需要缓存元数据）
* chunkserver中的chunk被作为本地文件存储，Linux系统已经在内存中对经常访问的数据在缓冲区缓存，因此也不需要额外地缓存文件数据。

**一致性模型：**

GFS`宽松`的一致性模型(relaxed consistency model)可以很好地支持我们的高度分布式应用程序，且实现起来简单高效。

文件`命名空间`的变更（例如创建文件）操作是`原子性`的。它们仅由master处理。`命名空间锁`保证了原子性和正确性；master的操作日志定义了这些操作的全局总顺序。

`文件区域（file region）`的一致性状态有下面几种：

* `consistent（一致的）`：如果一个文件区域的任意一个副本被任何client读取总能得到相同的数据，那么这个文件区域状态为consistent（一致的）
* `defined（确定的）`：在一个文件区域的数据变更后，如果它是一致的，且client总能看到其写入的内容，那么这个文件区域的状态为defined（确定的）
    * defined状态包含了consistent状态
* `consistent but undefined（一致的但非确定的）`：文件区域在并发变更执行后的状态为consistent but undefined（一致的但非确定的）
    * 所有客户端能看到同样的数据，但数据可能并不反映任何一个`变更写入`的数据。

通常，数据融合了多个变更的内容。文件区域在一个失败的变更后状态会变为`inconsistent（不一致的）`（且undefined）：不同client在不同时刻可能看到不同的数据。

### 2.3. 读操作简要流程

结合上述架构图，说明下“读”操作的流程：

1. 首先，通过固定的chunk大小，client将应用程序指定的`文件名`和`chunk偏移量`翻译为该文件中的`chunk index`（块序号）
2. 然后，client向master发送一个包含了文件名和`chunk index`的请求
3. master会返回其相应的`chunk handle`和副本所在的位置
4. client将这个信息以文件名和`chunk index`为键进行**缓存**
5. client接着向最有可能为最近的副本所在的chunkserver发送请求。请求中指定了`chunk handle`和`byte range`。
6. 之后，client再次读取相同的chunk时不再需要与master交互，直到缓存过期或文件被重新打开

事实上，client通常会在同一个请求中请求多个chunk，master也可以返回包含多个chunk的响应。这种方式避免了client与master进一步的通信，在几乎不需要额外开销的情况下得到更多的信息。

### 2.4. 系统交互

#### 2.4.1. 租约

改变chunk或元数据的操作被称为“`变更`”，如`write`或`append`，chunk变更时，其每个副本都会应用变更。

GFS使用`租约（lease）`来维护副本间变更顺序的一致性。

chunk变更时，master向其中一个`副本`授权一个变更的`租约`，该副本称为 **`primary`**（有时也代指primary副本所在的chunkserver）。

`primary`为该chunk的其他副本来应用变更。这种租约机制是为了最小化master的负载设计的。

#### 2.4.2. 写操作简要流程

上面说明了读操作的流程，这里讲下写操作流程。

先看下`write`操作的控制和数据流示意图：  
![写操作控制和数据流](/images/gfs-write-control-data-flow.png)

下面对上图的`write`各步骤进行说明（编号一一对应）：

1. 客户端向master询问哪个chunkserver持有指定chunk的租约和该chunk其他副本的位置。若没有chunkserver持有租约，则master选择一个副本对其授权租约
2. master返回该chunk对应的各副本位置，包含`primary`副本和其他副本（`secondary`）。client为后续的变更`缓存`这些信息。
3. client将数据推送到所有副本。每个chunkserver都会将数据在内部的LRU中**缓存**，直到数据被使用或缓存老化失效（age out）。
    * 如上图中所示，`控制流`和`数据流`是解耦的。在`控制流`从client向primary再向所有secondary推送的同时，`数据流`沿着一条精心挑选的`chunkserver链`以`流水线`的方式线性推送（链式传给离它最近的chunkserver）。
    * 即`数据流`的推送并不是client将数据推送多份。每台机器会将数据传递给在网络拓扑中`最近的`且`还没有收到数据`的机器。基于网络拓扑技术来提高数据流传输性能，与哪台chunkserver是primary无关。
    * GFS会通过 **`流水线`**的方式基于TCP连接传输数据，以最小化时延。当chunkserver收到一部分数据时，它会立刻开始将数据传递给其他chunkserver。由于交换网络是全双工的，发送数据不会减少接受数据的速度，，所以流水线可以大幅减少时延。
4. 一旦所有副本都确认收到了数据，client会向primary发送一个`write`请求。
    * 这个请求标识了之前推送到所有副本的数据的作用。
    * primary会为其收到的所有的变更分配连续的`编号`，这一步提供了重要的`顺序`，primary按照该顺序对本地应用该变更。
5. primary将write请求继续传递给其他secondary副本。每个secondary副本都按照primary分配的顺序来应用变更。
6. 所有的secondary副本通知primary其完成了变更操作。
7. primary回复client。
    * 任意副本遇到的任何错误都会被报告给client。错误发生时，write操作可能在primary或部分secondary子集中已经被成功执行了（如果primary中发生错误，则不会分配`顺序`，也不会继续传递下发给其他副本）
    * 只要错误发生，该请求都会被认为是失败的，且被修改的区域的状态为`inconsistent`
    * client中的代码会通过`重试`失败的变更来处理这种错误。首先它会重试几次步骤（3）到步骤（7），如果还没有成功，再从`write`请求的初始操作开始重试。

如果应用程序发出的一次write请求过大或跨多个chunk，GFS的client代码会将其**拆分**成多个write操作。

#### 2.4.3. record append原子操作

GFS提供了一种叫做`record append`的`原子性append`操作。在record append中，client仅需指定待追加的数据。GFS会为其选择一个偏移量，在该偏移量处`至少一次地`、`原子性地`将数据作为一个连续的字节序列追加到文件，并将该偏移量返回给client。这很像Unix系统中，在不存在多writer并发写入带来的竞态条件下，写入以`O_APPEND`模式打开的文件的情况。

record append是`变更`的一种，也遵循上述`write`写的控制流，仅在primary端稍有点额外的逻辑。在client将数据推送到所有副本的最后一个chunk之后，client会向primary发送一个请求。primary会检查当新记录追加到该chunk之后，是否会导致该chunk超过最大的chunk大小限制（`64MB`）。如果会超出chunk大小限制，primary会将该chunk**填充到最大的大小**，并通知secondary也做相同的操作，再回复客户端，使其在下一个chunk上重试该操作。

record append操作限制了每次最多写入`最大chunk大小的四分之一`的数据，以保证在最坏的情况下产生的**碎片**在可接受的范围内。

#### 2.4.4. 快照

`快照`操作**几乎会在瞬间**对一个文件或一个目录树（被称为源）完成拷贝，同时能够尽可能少地打断正在执行的变更。

用户使用快照操作来快速地对一个庞大的数据集的一个分支进行拷贝（或对其拷贝再进行拷贝等等），或者在变更前对当前状态创建检查点(checkpoint)，这样就可以轻松地提交或回滚该变更。

基于`写时复制`技术来实现快照。

* 当master收到快照请求的时候，它首先会`撤销`快照涉及到的文件的chunk上所有`未完成的租约`。这确保了对这些chunk在后续的写入时都需要与master交互以查找租约的持有者。
* 在租约被收回或过期后，master会将快照操作记录到日志中，并写入到磁盘。
* 随后，master会通过在内存中创建一个源文件或源目录树的`元数据的副本`的方式来进行快照操作。新创建的快照文件与源文件指向相同的chunk。

快照操作后，说明下对应的`write`操作。下面以`chunk C`为例（注意`C`和`C'`有区别）：

1. 在快照操作后，首次想要对`chunk C`进行write操作的client会向master发送一个请求以找到当前的租约持有者。
2. master会检测到`chunk C`的引用数超过1个，并会推迟对client的响应，且选取一个新的`chunk handler C'`
3. 接着，master请求每个当前持有`chunk C`副本的chunkserver去创建一个新`chunk C'`。通过在与源chunk相同的chunkserver上创建新chunk，可以保证数据只在本地拷贝，而不会通过网络拷贝。
4. 在这之后，请求的处理逻辑就与处理任何其他chunk的请求一样了：master向新`chunk C'`的一个副本授权租约并将其响应client的请求。
5. 这样，client就可以像平常一样对chunk进行write操作，且client并不知道这个chunk是刚刚从一个已有的chunk创建来的。

### 2.5. master操作

master执行所有命名空间操作。除此之外，master还管理整个系统中chunk的副本：master做chunk分配（placement）决策、创建新chunk与副本、协调各种系统范围的活动以保持chunk副本数饱和、平衡所有chunkserver的负载并回收未使用的存储。

#### 2.5.1. 命名空间管理和锁

* GFS在逻辑上用一个`完整路径名`到`元数据`的`查找表`来表示命名空间。通过`前缀压缩技术`，这个查找表可在内存中高效地表示。
* 在命名空间树上的每个节点（既可能是一个文件的绝对路径名，也可能是一个目录的绝对路径名）都有一个与之关联的`读写锁（read-write lock）`
* master的每个操作执行前都会请求一系列的锁。如果master的操作包含命名空间`/d1/d2/…/dn/leaf`，master会在目录`/d1`、`/d1/d2`，…，`/d1/d2/…/dn`上请求**读取锁**，并在完整路径名`/d1/d2/…/dn/leaf`上请求**读取锁**或**写入锁**
* 快照操作时，如果有文件创建操作，由于加锁冲突，两个操作会保证串行。
* 这种锁机制提供了一个非常好的性质：允许在**同一目录**下`并发`地执行变更。例如，在同一目录下的多个文件创建操作可以并发执行：每个文件创建操作都获取其父目录的读取锁与被创建的文件的写入锁。目录名上的读取锁足够防止其被删除、重命名或快照。文件名上的写入锁可以防止相同同名文件被创建两次。此外，为了防止死锁，锁的`获取顺序`总是一致的：首先按照命名空间树中的层级排序，在同一层级内按照字典顺序排序。

#### 2.5.2. 副本分配

* chunk副本分配策略有两个目标：最大化数据可靠性和可用性、最大化网络带宽的利用。
* GFS在机架间分散chunk的副本。这样可以保证整个机架异常时，chunk的一些副本仍然存在并保持可用状态；同时对chunk的流量（尤其是**读流量**）可以充分利用各个机架的网络带宽。
    * 而另一方面，**写流量**必须流经多个机架，这是设计上做出的权衡（tradeoff）。
  
#### 2.5.3. chunk创建、重做副本、重均衡

chunk副本的创建可能由三个原因引起：`chunk创建`、`重做副本（re-replication）`和`重均衡（rebalancing）`。

**chunk创建（chunk creation）：**

创建chunk时需要选择空副本的位置，位置的选择会考虑很多因素：

* 我们希望在磁盘利用率低于平均值的chunkserver上放置副本，随着时间推移，这样将平衡chunkserver间的磁盘利用率。
* 我们希望限制每台chunkserver上最近创建的chunk的数量。尽管创建chunk本身的开销很小，但是master还要会可靠的预测即将到来的大量的写入流量。
* 另外，按上面讨论的副本分配原则，我们希望将chunk的副本`跨机架`分散。

**重做副本（re-replication）：**

当chunk可用的副本数**少于**用户设定的目标值时，master会重做副本（re-replication）。每个需要重做副本的chunk会参考一些因素按照`优先级`排序，比如副本数量离目标数量多的优先、非删除状态的文件chunk优先、被阻塞的client对应的chunk优先等。

master选取优先级最高的chunk，并向若干chunkserver发送命令让其直接从一个存在且合法的副本来拷贝这个chunk的数据。新副本位置的选取与创建新chunk时位置选取的目标类似：均衡磁盘空间利用率、限制在单个chunkserver上活动的克隆操作数、在机架间分散副本。

**重均衡（rebalancing）：**

每隔一段时间master会对副本进行重均衡：master会检测当前的副本分布并移动副本位置，使磁盘空间和负载更加均衡。

在这个过程中，master会逐渐填充一个新的chunkserver，而不会立刻让来自新chunk的高负荷的写入流量压垮新的chunkserver。新副本放置位置的选择方法与我们上文中讨论过的类似。

此外，master必须删除一个已有副本。通常，master会选择删除空闲磁盘空间低于平均的chunkserver上的副本，以均衡磁盘空间的使用。

## 3. 小结


## 4. 参考

1、[GFS论文](https://pdos.csail.mit.edu/6.824/papers/gfs.pdf)

2、[《The Google File System》论文翻译（GFS-SOSP2003）](https://blog.mrcroxx.com/posts/paper-reading/gfs-sosp2003/)

3、[视频：Lecture 2 - GFS](https://www.bilibili.com/video/BV1R7411t71W?p=3&vd_source=477b80445c7c1a81617bbea3bdf9a3c1)

4、[课程中文翻译：Lecture 03 - GFS](https://mit-public-courses-cn-translatio.gitbook.io/mit6-824/lecture-03-gfs)

5、[MIT 6.824 2020 视频笔记三：GFS](https://www.qtmuniao.com/2020/03/14/6-824-vidoe-notes-3-gfs/)
